{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Infra Lab","text":"<p>This project is designed as a hands-on learning experience to set up an on-premise Kubernetes cluster using KVM. It covers the deployment of core Kubernetes services and demonstrates how to automate infrastructure setup and application delivery using Infrastructure as Code (IaC) and GitOps principles, leveraging tools like Terraform and ArgoCD.</p> <p>Every step of the process\u2014from planning and designing the cluster architecture to manually configuring each component\u2014has been carefully documented. These guides are available in the documentation section, making it easy for anyone to replicate and build their own home Kubernetes cluster.</p>"},{"location":"#directory-hierarchy","title":"Directory Hierarchy","text":"<pre><code>.\n\u251c\u2500\u2500 addons\n\u2502   \u2514\u2500\u2500 bind9\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 getting-started\n\u2502   \u251c\u2500\u2500 storage\n\u2502   \u2514\u2500\u2500 stylesheets\n\u251c\u2500\u2500 jarvis-kvm\n\u2502   \u251c\u2500\u2500 ansible\n\u2502   \u2514\u2500\u2500 terraform\n\u251c\u2500\u2500 k0s\n\u2502   \u2514\u2500\u2500 k0sctl.yaml\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p><code>.git-crypt</code>: contains gpg files of collaborators who can open encrypted git-crypt file.</p> <p><code>jarvis-kvm</code>: terraform code to provision VMs and other resources on top KVM hypervisor.</p>"},{"location":"#technology-stack","title":"Technology Stack","text":"<p>The following picture shows the high level components of opensource solutions used so far in the cluster, which installation process has been documented and its deployment has been automated with Open Tofu:</p> <p> </p> Name Description ArgoCD GitOps tool Cloud-init Automate OS initial installation Ceph Distributed Storage Containerd Container runtime integrated with K0S Calico Kubernetes Networking (CNI) and Load Balancer CoreDNS Kubernetes DNS Debian Cluster nodes OS &amp; Host OS ExternalDNS External DNS synchronization Cert-manager TLS Certificates management K0S  The simple, solid &amp; certified Kubernetes distribution that works on any infrastructure KVM  Full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V) MetalLB Load-balancer implementation for bare metal Kubernetes clusters"},{"location":"addons/bind9/","title":"Index","text":""},{"location":"addons/bind9/#generate-tsig-key","title":"Generate TSIG Key","text":"<p><pre><code>tsig-keygen -a hmac-sha512 externaldns-key &gt; config/keys/external-dns.key\n</code></pre> The output should be like this: <pre><code>key \"externaldns-key\" {\n    algorithm hmac-sha512;\n    secret \"&lt;BASE64 encoded&gt;\";\n};\n</code></pre></p>"},{"location":"addons/bind9/#run-bind9","title":"Run BIND9","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/","title":"BIND9 Deployment Guide","text":""},{"location":"docs/external-services/bind9/deployment/#overview","title":"Overview","text":"<p>This guide provides step-by-step instructions for deploying BIND9 DNS server on the bastion host (bastion01) using Docker Compose. The deployment includes configuration setup, TSIG key generation, and integration with the lab infrastructure.</p>"},{"location":"docs/external-services/bind9/deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/external-services/bind9/deployment/#system-requirements","title":"System Requirements","text":"Component Requirement Verification Command Docker 20.10+ <code>docker --version</code> Docker Compose 2.0+ <code>docker compose version</code>"},{"location":"docs/external-services/bind9/deployment/#configuration-deployment","title":"Configuration Deployment","text":""},{"location":"docs/external-services/bind9/deployment/#step-1-prepare-deployment-directory","title":"Step 1: Prepare Deployment Directory","text":"<pre><code># Create BIND9 deployment directory\nmkdir -p /opt/bind9/{config,zones,cache,config/keys}\n\n# Set proper ownership\nchown -R cloud:cloud /opt/bind9\n\n# Switch back to cloud user\nsu - cloud\ncd /opt/bind9\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-2-generate-tsig-key","title":"Step 2: Generate TSIG Key","text":"<pre><code># Generate authentication key for dynamic DNS updates\ntsig-keygen -a hmac-sha512 externaldns-key &gt; /opt/bind9/config/keys/external-dns.key\n\n# Verify key generation\ncat config/keys/external-dns.key\n\n# Expected output format:\n# key \"externaldns-key\" {\n#     algorithm hmac-sha256;\n#     secret \"base64-key-string==\";\n# };\n</code></pre> <p>Key Security</p> <p>Store the generated key securely. This key will be used by External-DNS for dynamic updates.</p>"},{"location":"docs/external-services/bind9/deployment/#step-3-create-configuration-files","title":"Step 3: Create Configuration Files","text":""},{"location":"docs/external-services/bind9/deployment/#main-configuration","title":"Main Configuration","text":"<pre><code>cat &gt; config/named.conf &lt;&lt; 'EOF'\ninclude \"/etc/bind/named.conf.options\";\ninclude \"/etc/bind/named.conf.local\";\nEOF\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#server-options","title":"Server Options","text":"<pre><code>cat &gt; config/named.conf.options &lt;&lt; 'EOF'\n//The following keys are used for dynamic DNS updates\ninclude \"/etc/bind/keys/external-dns.key\";\n\noptions {\n    directory \"/var/cache/bind\";\n    recursion yes;\n    allow-query { any; };\n\n    forwarders {\n        8.8.8.8;\n        1.1.1.1;\n    };\n\n    dnssec-validation auto;\n\n    listen-on { any; };\n};\n\nlogging {\n   channel stdout_channel {\n       stderr;\n       severity info;\n       print-category yes;\n       print-severity yes;\n       print-time yes;\n   };\n   category default {\n       stdout_channel;\n   };\n   category queries {\n       stdout_channel;\n   };\n   category security {\n       stdout_channel;\n   };\n   category dnssec {\n       stdout_channel;\n   };\n};\nEOF\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#zone-configuration","title":"Zone Configuration","text":"<pre><code>cat &gt; config/named.conf.local &lt;&lt; 'EOF'\nzone \"lab.riupie.com\" {\n    type master;\n    file \"/zones/lab.riupie.com.zone\";\n    allow-transfer {\n        key \"externaldns-key\";\n    };\n    update-policy {\n        grant externaldns-key zonesub any;\n    };\n};\nEOF\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-3-create-zone-file","title":"Step 3: Create Zone File","text":"<pre><code># Create initial zone file with current date as serial\nSERIAL=$(date +%Y%m%d%H)\n\ncat &gt; zones/lab.riupie.com.zone &lt;&lt; EOF\n\\$TTL 3600  ; 1 hour\nlab.riupie.com.     IN SOA  ns1.lab.riupie.com. admin.lab.riupie.com. (\n                ${SERIAL} ; serial\n                3600       ; refresh (1 hour)\n                1800       ; retry (30 minutes)\n                604800     ; expire (1 week)\n                86400      ; minimum (1 day)\n                )\n            NS  ns1.lab.riupie.com.\n\nns1.lab.riupie.com.     A       192.168.10.9\nEOF\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-4-create-docker-compose-file","title":"Step 4: Create Docker Compose File","text":"<pre><code>cat &gt; docker-compose.yaml &lt;&lt; 'EOF'\nservices:\n  bind9:\n    image: internetsystemsconsortium/bind9:9.20\n    container_name: bind9\n    ports:\n      - \"53:53/udp\"\n      - \"53:53/tcp\"\n    volumes:\n      - ./config:/etc/bind\n      - ./zones:/zones\n      - ./cache:/var/cache/bind\n    restart: unless-stopped\n    environment:\n      - TZ=UTC\n    healthcheck:\n      test: [\"CMD\", \"dig\", \"@localhost\", \"lab.riupie.com\", \"SOA\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\nEOF\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#configuration-validation","title":"Configuration Validation","text":""},{"location":"docs/external-services/bind9/deployment/#step-1-validate-configuration-files","title":"Step 1: Validate Configuration Files","text":"<pre><code># Check BIND configuration syntax\n docker run --rm -v \"$(pwd)/config:/etc/bind\" \\\n   --entrypoint named-checkconf \\\n   internetsystemsconsortium/bind9:9.20 \\\n   /etc/bind/named.conf\n\n# Expected output: (no output means configuration is valid)\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-2-validate-zone-file","title":"Step 2: Validate Zone File","text":"<pre><code># Check zone file syntax\ndocker run --rm \\\n  -v \"$(pwd):/bind-data\" \\\n  --entrypoint named-checkzone \\\n  internetsystemsconsortium/bind9:9.20 \\\n  lab.riupie.com /bind-data/zones/lab.riupie.com.zone\n\n# Expected output:\n# zone lab.riupie.com/IN: loaded serial 2025050307\n# OK\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#service-deployment","title":"Service Deployment","text":""},{"location":"docs/external-services/bind9/deployment/#step-1-start-bind9-service","title":"Step 1: Start BIND9 Service","text":"<pre><code># Start the DNS server\ndocker compose up -d\n\n# Verify container is running\ndocker compose ps\n\n# Expected output:\n# NAME      IMAGE                                    COMMAND             SERVICE   CREATED         STATUS                   PORTS\n# bind9     internetsystemsconsortium/bind9:9.20    \"/usr/sbin/named\"   bind9     2 seconds ago   Up 1 second (healthy)   0.0.0.0:53-&gt;53/tcp, 0.0.0.0:53-&gt;53/udp\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-2-monitor-startup-logs","title":"Step 2: Monitor Startup Logs","text":"<pre><code># Check container logs\ndocker compose logs -f bind9\n\n# Expected log entries:\n# bind9  | starting BIND 9.20.3 (Extended Support Version)\n# bind9  | zone lab.riupie.com/IN: loaded serial 2025050307\n# bind9  | running\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-3-test-dns-resolution","title":"Step 3: Test DNS Resolution","text":"<pre><code># Test internal zone resolution\ndig @localhost lab.riupie.com SOA\n\n# Expected output includes:\n# ;; ANSWER SECTION:\n# lab.riupie.com. 3600 IN SOA ns1.lab.riupie.com. admin.lab.riupie.com. ...\n\n# Test recursive resolution\ndig @localhost google.com A\n\n# Should return Google's IP addresses\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#dynamic-dns-integration","title":"Dynamic DNS Integration","text":""},{"location":"docs/external-services/bind9/deployment/#step-1-extract-tsig-key-for-external-dns","title":"Step 1: Extract TSIG Key for External-DNS","text":"<pre><code># On bastion01, extract the TSIG key secret\ncd /opt/bind9\ngrep secret config/keys/external-dns.key\n\n# Output format:\n# secret \"base64-encoded-key-here==\";\n\n# Copy this key for External-DNS configuration in Kubernetes\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#step-2-test-dynamic-dns-updates","title":"Step 2: Test Dynamic DNS Updates","text":"<pre><code># Create a test update file\ncat &gt; test-update.txt &lt;&lt; 'EOF'\nserver 192.168.10.9\nzone lab.riupie.com.\nupdate add test.lab.riupie.com. 300 A 192.168.10.100\nsend\nEOF\n\n# Test dynamic update (requires TSIG key setup)\nnsupdate -k config/keys/external-dns.key &lt; test-update.txt\n\n# Verify the update\ndig @192.168.10.9 test.lab.riupie.com A\n</code></pre>"},{"location":"docs/external-services/bind9/deployment/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>On Debian-based systems, systemd-resolved may conflict with BIND9. You can resolve this by pointing systemd-resolved to your BIND9 server. Edit <code>/etc/systemd/resolved.conf</code>:</p> <pre><code>[Resolve]\nDNS=127.0.0.1   # or your BIND9 server IP\nDNSStubListener=no\n</code></pre> <p>Then restart the service:</p> <pre><code>sudo systemctl restart systemd-resolved\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/","title":"Infrastructure Deployment","text":""},{"location":"docs/getting-started/infrastructure-deployment/#overview","title":"Overview","text":"<p>This guide walks through the automated deployment of virtual infrastructure using OpenTofu (Terraform alternative). The deployment creates a complete virtualized environment with networking, storage, and virtual machines ready for Kubernetes installation.</p>"},{"location":"docs/getting-started/infrastructure-deployment/#what-will-be-deployed","title":"What Will Be Deployed","text":"Component Description Configuration Virtual Network NAT bridge for VM connectivity <code>virbr1</code> - 192.168.10.0/24 &amp; <code>virbr2</code> - 192.168.11.0/24 Storage Pool Disk storage for VM images <code>/var/lib/libvirt/images/</code> Virtual Machines 5 VMs for the lab environment 1 bastion + 1 control plane + 3 workers"},{"location":"docs/getting-started/infrastructure-deployment/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have completed the Prerequisites guide.</p>"},{"location":"docs/getting-started/infrastructure-deployment/#repository-setup","title":"Repository Setup","text":""},{"location":"docs/getting-started/infrastructure-deployment/#clone-the-repository","title":"Clone the Repository","text":"<pre><code># Clone the infra-lab repository\ngit clone https://github.com/riupie/infra-lab.git\ncd infra-lab\n\n# Verify repository structure\nls -la\n</code></pre> <p>Expected Directory Structure: <pre><code>infra-lab/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 jarvis-kvm/\n\u2502   \u251c\u2500\u2500 ansible/\n\u2502   \u2514\u2500\u2500 terraform\n\u2502       \u251c\u2500\u2500 networks\n\u2502       \u251c\u2500\u2500 storage-pool\n\u2502       \u251c\u2500\u2500 tf-state\n\u2502       \u2514\u2500\u2500 vm\n\u2514\u2500\u2500 k0s/\n</code></pre></p>"},{"location":"docs/getting-started/infrastructure-deployment/#network-deployment","title":"Network Deployment","text":""},{"location":"docs/getting-started/infrastructure-deployment/#step-1-deploy-virtual-network","title":"Step 1: Deploy Virtual Network","text":"<pre><code># Navigate to network configuration\ncd jarvis-kvm/terraform/networks\n\n# Initialize OpenTofu\ntofu init\n\n# Review the planned changes\ntofu plan\n\n# Apply network configuration\ntofu apply\n</code></pre> <p>What this creates: - Virtual bridge (<code>virbr1</code> and <code>virbr2</code>) with NAT configuration - Network subnet: <code>192.168.10.0/24</code> (net-lab), <code>192.168.11.0/24</code> (ceph-lab) - Gateway: <code>192.168.10.1</code>, <code>192.168.11.1</code></p>"},{"location":"docs/getting-started/infrastructure-deployment/#verify-network-creation","title":"Verify Network Creation","text":"<pre><code># Check network status\nvirsh net-list\n\n# Expected output:\n# Name       State    Autostart   Persistent\n#---------------------------------------------\n# ceph-lab   active   yes         yes\n# net-lab    active   yes         yes\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/#storage-deployment","title":"Storage Deployment","text":""},{"location":"docs/getting-started/infrastructure-deployment/#step-2-create-storage-pool","title":"Step 2: Create Storage Pool","text":"<pre><code># Navigate to storage pool configuration\ncd ../terraform/storage-pool\n\n# Initialize and apply\ntofu init\ntofu plan\ntofu apply\n</code></pre> <p>What this creates: - Libvirt storage pool named <code>default</code> - Storage location: <code>/var/lib/libvirt/images/</code> - Automatic permission management for libvirt - Directory structure for VM disk images</p>"},{"location":"docs/getting-started/infrastructure-deployment/#verify-storage-pool","title":"Verify Storage Pool","text":"<pre><code># Check storage pool status\nvirsh pool-list\n\n# Expected output:\n# Name      State    Autostart   Persistent\n# default   active   yes         yes\n\n# Check storage pool details\nvirsh pool-info default\n\n# Verify storage location\nls -la /var/lib/libvirt/images/\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/#virtual-machine-deployment","title":"Virtual Machine Deployment","text":""},{"location":"docs/getting-started/infrastructure-deployment/#step-3-deploy-vms","title":"Step 3: Deploy VMs","text":"<pre><code># Navigate to VM configuration\ncd ../vm\n\n# Initialize OpenTofu\ntofu init\n\n# Review VM specifications (optional)\ntofu plan\n\n# Deploy VMs\ntofu apply\n</code></pre> <p>VM Specifications:</p> VM Name Role vCPUs Memory Disk IP Address bastion01 DNS + Tailscale Router 2 4GB 40GB 192.168.10.9 master01 Kubernetes Control Plane 2 4GB 40GB 192.168.10.10 worker01 Kubernetes Worker 2 8GB 40GB 192.168.10.11 worker02 Kubernetes Worker 2 8GB 40GB 192.168.10.12 worker03 Kubernetes Worker 2 8GB 40GB 192.168.10.13"},{"location":"docs/getting-started/infrastructure-deployment/#deployment-process","title":"Deployment Process","text":"<p>The VM deployment will: 1. Create VM Disks: Individual disk images for each VM 2. Configure Cloud-Init: User accounts and SSH keys 3. Network Assignment: Static IP addresses in the lab network 4. Start VMs: Boot all virtual machines</p>"},{"location":"docs/getting-started/infrastructure-deployment/#verification","title":"Verification","text":""},{"location":"docs/getting-started/infrastructure-deployment/#step-4-verify-vm-deployment","title":"Step 4: Verify VM Deployment","text":""},{"location":"docs/getting-started/infrastructure-deployment/#check-vm-status","title":"Check VM Status","text":"<pre><code># Check VM status\nvirsh list --all\n\n# Expected output:\n# Id   Name        State\n# 1    bastion01   running\n# 2    master01    running\n# 3    worker01    running\n# 4    worker02    running\n# 5    worker03    running\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/#test-network-connectivity","title":"Test Network Connectivity","text":"<pre><code># Test VM connectivity\nping -c 3 192.168.10.9   # bastion01\nping -c 3 192.168.10.10  # master01\nping -c 3 192.168.10.11  # worker01\nping -c 3 192.168.10.12  # worker02\nping -c 3 192.168.10.13  # worker03\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/#ssh-access-test","title":"SSH Access Test","text":"<pre><code># Test SSH access (after VMs are fully booted)\nssh cloud@192.168.10.9   # bastion01\nssh cloud@192.168.10.10  # master01\n\n# If SSH fails, VMs may still be booting\n# Check VM console:\nvirsh console bastion01\n</code></pre>"},{"location":"docs/getting-started/infrastructure-deployment/#next-steps","title":"Next Steps","text":"<p>Once infrastructure deployment is complete:</p> <ol> <li>Kubernetes Setup - Deploy k0s Kubernetes cluster</li> <li>Storage Configuration - Set up Ceph storage</li> <li>Service Configuration - Configure DNS, monitoring, and other services</li> </ol>"},{"location":"docs/getting-started/kubernetes-setup/","title":"Kubernetes Setup","text":""},{"location":"docs/getting-started/kubernetes-setup/#overview","title":"Overview","text":"<p>This guide covers the installation and configuration of a Kubernetes cluster using k0s, a lightweight and CNCF-certified Kubernetes distribution. The setup uses k0sctl for cluster lifecycle management and provides a production-ready environment.</p>"},{"location":"docs/getting-started/kubernetes-setup/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have completed:</p> <ol> <li>Prerequisites - System requirements and software installation</li> <li>Infrastructure Deployment - VM and network setup</li> </ol>"},{"location":"docs/getting-started/kubernetes-setup/#verify-infrastructure","title":"Verify Infrastructure","text":"<pre><code># Verify all VMs are running\nvirsh list\n\n# Test connectivity to all nodes\nping -c 2 192.168.10.9   # bastion01\nping -c 2 192.168.10.10  # master01\nping -c 2 192.168.10.11  # worker01\nping -c 2 192.168.10.12  # worker02\nping -c 2 192.168.10.13  # worker03\n\n# Test SSH access\nssh cloud@192.168.10.10 \"hostname &amp;&amp; uptime\"\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#k0sctl-installation-from-bastion","title":"k0sctl Installation (from Bastion)","text":""},{"location":"docs/getting-started/kubernetes-setup/#install-k0sctl","title":"Install k0sctl","text":"<p>k0sctl is the command-line tool for managing k0s Kubernetes clusters.</p> <pre><code># Method 1: Using the install script (recommended)\ncurl -sSLf https://get.k0s.sh | sudo sh\n\n# Method 2: Manual installation\nwget https://github.com/k0sproject/k0sctl/releases/download/v0.15.5/k0sctl-linux-x64\nchmod +x k0sctl-linux-x64\nsudo mv k0sctl-linux-x64 /usr/local/bin/k0sctl\n\n# Verify installation\nk0sctl version\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#cluster-configuration","title":"Cluster Configuration","text":""},{"location":"docs/getting-started/kubernetes-setup/#download-configuration","title":"Download Configuration","text":"<pre><code># Download the k0sctl configuration\nwget https://raw.githubusercontent.com/riupie/infra-lab/refs/heads/main/k0s/k0sctl.yaml\n\n# Review the configuration\ncat k0sctl.yaml\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#configuration-overview","title":"Configuration Overview","text":"<p>The k0sctl configuration defines:</p> Component Node IP Address Role Control Plane master01 192.168.10.10 Controller Worker Node worker01 192.168.10.11 Worker Worker Node worker02 192.168.10.12 Worker Worker Node worker03 192.168.10.13 Worker"},{"location":"docs/getting-started/kubernetes-setup/#cluster-deployment","title":"Cluster Deployment","text":""},{"location":"docs/getting-started/kubernetes-setup/#deploy-the-cluster","title":"Deploy the Cluster","text":"<pre><code># Apply the cluster configuration\nk0sctl apply --config k0sctl.yaml\n\n# Monitor deployment progress with debug output\nk0sctl apply --config k0sctl.yaml --debug\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#deployment-process","title":"Deployment Process","text":"<p>The deployment performs the following steps:</p> <ol> <li>Connectivity Check: Validates SSH access to all nodes</li> <li>System Preparation: Installs k0s binary on all nodes</li> <li>Control Plane Init: Initializes the Kubernetes control plane</li> <li>Worker Join: Joins worker nodes to the cluster</li> <li>Network Setup: Configures Calico CNI</li> <li>Health Check: Verifies cluster functionality</li> </ol>"},{"location":"docs/getting-started/kubernetes-setup/#cluster-verification","title":"Cluster Verification","text":""},{"location":"docs/getting-started/kubernetes-setup/#get-cluster-access","title":"Get Cluster Access","text":"<pre><code># Generate kubeconfig\nmkdir ~/.kube\nk0sctl kubeconfig --config k0sctl.yaml &gt; ~/.kube/config\n\n# Verify kubectl access\nkubectl cluster-info\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#verify-cluster-status","title":"Verify Cluster Status","text":""},{"location":"docs/getting-started/kubernetes-setup/#check-node-status","title":"Check Node Status","text":"<pre><code># List all nodes\nkubectl get nodes\n\n# Expected output:\n# NAME       STATUS   ROLES                  AGE   VERSION\n# master01   Ready    control-plane,worker   5m    v1.33.1+k0s\n# worker01   Ready    worker                 4m    v1.33.1+k0s\n# worker02   Ready    worker                 4m    v1.33.1+k0s\n# worker03   Ready    worker                 4m    v1.33.1+k0s\n\n# Check node details\nkubectl get nodes -o wide\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#verify-system-pods","title":"Verify System Pods","text":"<pre><code># Check system pods status\nkubectl get pods -n kube-system\n\n# Expected pods:\n# - calico-* (CNI networking)\n# - coredns-* (DNS resolution)\n# - konnectivity-* (API server connectivity)\n# - metrics-server-* (resource metrics)\n\n# Check pod status across all namespaces\nkubectl get pods --all-namespaces\n</code></pre>"},{"location":"docs/getting-started/kubernetes-setup/#next-steps","title":"Next Steps","text":"<p>After successful Kubernetes setup:</p> <ol> <li>Storage Integration - Configure Ceph storage</li> <li>CI/CD - Set up GitOps with ArgoCD</li> </ol>"},{"location":"docs/getting-started/lab-architecture/","title":"Lab Architecture","text":""},{"location":"docs/getting-started/lab-architecture/#overview","title":"Overview","text":"Infrastructure Lab Architecture Diagram <p>This document provides a comprehensive overview of the network configuration and virtual machine layout for a KVM-based home lab environment. The architecture uses libvirt for virtualization management on a Linux host and is designed for learning, development, and testing purposes.</p>"},{"location":"docs/getting-started/lab-architecture/#architecture-highlights","title":"Architecture Highlights","text":"<ul> <li>Hypervisor: KVM with libvirt management</li> <li>Network Design: NAT-based virtual networks with custom bridges</li> <li>Service Discovery: Internal DNS with Tailscale integration</li> <li>Orchestration: Kubernetes cluster for container workloads</li> <li>Remote Access: Tailscale subnet routing for secure external access</li> </ul>"},{"location":"docs/getting-started/lab-architecture/#host-network-configuration","title":"Host Network Configuration","text":"<p>The host machine (<code>jarvis</code>) manages multiple network interfaces for VM connectivity and external access.</p>"},{"location":"docs/getting-started/lab-architecture/#network-interface-details","title":"Network Interface Details","text":"Interface Type Purpose Configuration <code>enp8s0</code> Physical NIC LAN connectivity Connected to home/office network <code>virbr1</code> Virtual Bridge VM internal network Subnet: <code>192.168.10.0/24</code> <code>virbr2</code> Virtual Bridge VM storage internal network Subnet: <code>192.168.11.0/24</code> <code>vnetX</code> Virtual TAP VM network adapters Auto-created by libvirt, attached to <code>virbr1</code> and <code>virbr2</code>"},{"location":"docs/getting-started/lab-architecture/#virtual-machine-inventory","title":"Virtual Machine Inventory","text":"<p>The lab environment consists of 5 virtual machines, each serving specific roles in the infrastructure:</p> VM ID Hostname Status Role IP Address Interface Resources 2 bastion01 Running DNS Server + Tailscale Router <code>192.168.10.15</code> vnet4 2 vCPU, 4GB RAM 3 master01 Running Kubernetes Control Plane <code>192.168.10.10</code> vnet5 2 vCPU, 4GB RAM 4 worker01 Running Kubernetes Worker Node <code>192.168.10.11</code> vnet6 2 vCPU, 8GB RAM 5 worker02 Running Kubernetes Worker Node <code>192.168.10.12</code> vnet7 2 vCPU, 8GB RAM 6 worker03 Running Kubernetes Worker Node <code>192.168.10.13</code> vnet9 2 vCPU, 8GB RAM"},{"location":"docs/getting-started/lab-architecture/#vm-connectivity","title":"VM Connectivity","text":"<ul> <li>Internal Communication: All VMs connected via <code>virbr1</code> bridge (192.168.10.0/24)</li> <li>External Access: NAT through host machine</li> <li>Service Discovery: Internal DNS provided by bastion01</li> <li>Remote Access: Tailscale subnet routing for external connectivity</li> </ul>"},{"location":"docs/getting-started/lab-architecture/#network-architecture","title":"Network Architecture","text":""},{"location":"docs/getting-started/lab-architecture/#nat-based-networking","title":"NAT-based Networking","text":"<p>The lab uses NAT (Network Address Translation) mode for VM connectivity, providing isolation and security:</p> Feature Description Note Private Subnet VMs operate on <code>192.168.10.0/24</code> and <code>192.168.11.0/24</code> Network isolation from external networks Internet Access Outbound connectivity via host VMs can reach external services External Isolation No direct inbound access from LAN Enhanced security posture"},{"location":"docs/getting-started/lab-architecture/#access-patterns","title":"Access Patterns","text":"<pre><code>Direction    | Source        | Destination   | Status\n-------------|---------------|---------------|--------\nOutbound     | VMs           | Internet      | \u2705 Allowed\nInternal     | VM \u2194 VM       | Internal IPs  | \u2705 Allowed\nHost Access  | VMs \u2194 Host    | Host Bridge   | \u2705 Allowed\nInbound      | LAN \u2192 VMs     | VM IPs        | \u274c Blocked (NAT)\n</code></pre> <p>Note</p> <p>External access to VMs requires either port forwarding or Tailscale subnet routing.</p>"},{"location":"docs/getting-started/lab-architecture/#infrastructure-services","title":"Infrastructure Services","text":""},{"location":"docs/getting-started/lab-architecture/#bastion-host-bastion01","title":"Bastion Host (bastion01)","text":"<p>The bastion host provides critical infrastructure services for the lab environment:</p>"},{"location":"docs/getting-started/lab-architecture/#internal-dns-server-bind9","title":"Internal DNS Server (BIND9)","text":"Service Configuration Purpose DNS Software BIND9 Authoritative DNS for internal zone Zone <code>*.lab.riupie.com</code> Internal service discovery Forwarders 1.1.1.1, 8.8.8.8 External DNS resolution Clients All lab VMs Centralized name resolution"},{"location":"docs/getting-started/lab-architecture/#tailscale-integration","title":"Tailscale Integration","text":"Feature Configuration Benefit Subnet Router Advertises <code>192.168.10.0/24</code> Remote access to entire lab Exit Node Optional internet routing Secure external connectivity Authentication Tailnet integration SSO-based access control Encryption WireGuard protocol Zero-trust network security"},{"location":"docs/getting-started/lab-architecture/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"docs/getting-started/lab-architecture/#production-host-specifications","title":"Production Host Specifications","text":"<p>The lab runs on a dedicated bare-metal server with the following specifications:</p> Component Specification Usage CPU AMD Ryzen 5 3600 (12 cores @ 3.6GHz) VM compute resources Memory 64GB DDR4 VM memory allocation Storage SSD storage pool VM disk images and data Network 1Gbps Ethernet Internet and LAN connectivity OS Debian GNU/Linux 12 (bookworm) KVM hypervisor host"},{"location":"docs/getting-started/lab-architecture/#host-system-details","title":"Host System Details","text":"<pre><code># System Information\nOS: Debian GNU/Linux 12 (bookworm) x86_64\nKernel: 6.1.0-28-amd64\nCPU: AMD Ryzen 5 3600 (12) @ 3.600GHz\nMemory: 64GB DDR4\nVirtualization: KVM with libvirt\n</code></pre>"},{"location":"docs/getting-started/lab-architecture/#minimum-requirements","title":"Minimum Requirements","text":"<p>For testing or development environments, you can use alternative hardware:</p> Component Minimum Recommended Notes CPU 4 cores 8+ cores Must support virtualization (VT-x/AMD-V) Memory 16GB 32GB+ 4GB per VM + host overhead Storage 100GB 500GB+ SSD recommended for performance Network 1Gbps 1Gbps+ For VM and container networking <p>Note</p> <p>Laptop or desktop systems can be used as long as they meet the minimum requirements and support hardware virtualization.</p>"},{"location":"docs/getting-started/prerequisites/","title":"Prerequisites","text":""},{"location":"docs/getting-started/prerequisites/#overview","title":"Overview","text":"<p>This document outlines the system requirements and software prerequisites needed to set up the infra-lab environment. Ensure all requirements are met before proceeding with the infrastructure deployment.</p>"},{"location":"docs/getting-started/prerequisites/#system-requirements","title":"System Requirements","text":""},{"location":"docs/getting-started/prerequisites/#minimum-requirements","title":"Minimum Requirements","text":"Component Minimum Recommended Notes CPU 4 cores 8+ cores Must support virtualization (VT-x/AMD-V) Memory 16GB 32GB+ 4GB per VM + host overhead Storage 100GB 500GB+ SSD recommended for performance Network 1Gbps 1Gbps+ For VM and container networking OS Debian/Ubuntu Linux Debian 12 (bookworm) Tested distributions"},{"location":"docs/getting-started/prerequisites/#virtualization-support","title":"Virtualization Support","text":"<p>Your CPU must support hardware virtualization:</p> <pre><code># Check for virtualization support\negrep -c '(vmx|svm)' /proc/cpuinfo\n\n# Should return &gt; 0 for Intel VT-x or AMD-V\n# If 0, check BIOS settings to enable virtualization\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#software-installation","title":"Software Installation","text":""},{"location":"docs/getting-started/prerequisites/#install-kvm-and-virtualization-tools","title":"Install KVM and Virtualization Tools","text":"<pre><code># Update package list\napt update\n\n# Install KVM and virtualization packages\napt -y install \\\n    qemu-kvm \\\n    libvirt-daemon-system \\\n    libvirt-daemon \\\n    virtinst \\\n    bridge-utils \\\n    libosinfo-bin \\\n    libguestfs-tools\n\n# Add user to libvirt group (if not root)\nusermod -aG libvirt $USER\n\n# Logout and login again to apply group changes\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#install-opentofu","title":"Install OpenTofu","text":"<p>OpenTofu is used for infrastructure automation and VM provisioning:</p> <pre><code># Method 1: Using package manager (recommended)\ncurl -Lo /tmp/tofu.deb https://github.com/opentofu/opentofu/releases/download/v1.6.0/tofu_1.6.0_amd64.deb\ndpkg -i /tmp/tofu.deb\n\n# Method 2: Manual installation\nwget https://github.com/opentofu/opentofu/releases/download/v1.6.0/tofu_1.6.0_linux_amd64.zip\nunzip tofu_1.6.0_linux_amd64.zip\nsudo mv tofu /usr/local/bin/\n\n# Verify installation\ntofu version\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#encrypt-terraform-state","title":"Encrypt terraform state","text":"<p>Terraform state file can be encrypted at rest by defining encryption configuration on backend configuration like this. For simplicity, I use PBKDF2 key to encrypt the state and then store the key on environment variable <code>TF_ENCRYPTION</code>. Store <code>TF_ENCRYPTION</code> on <code>.bashrc</code>/<code>.zshrc</code> (depends on your shell).</p> <pre><code>export TF_ENCRYPTION='key_provider \"pbkdf2\" \"key\" {\n     passphrase    = \"e7222fca0e62f64c0exxxxxxxxxxxxxxxxxx\"\n     key_length    = 32\n     salt_length   = 16\n     hash_function = \"sha256\"\n   }'\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#install-additional-tools","title":"Install Additional Tools","text":"<pre><code># Install packages required for VM image creation\napt install -y mkisofs xsltproc\n\n# Install helpful tools for VM management\napt install -y virt-manager virt-viewer\n\n# Verify installations\nwhich mkisofs xsltproc\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#verification","title":"Verification","text":""},{"location":"docs/getting-started/prerequisites/#verify-kvm-installation","title":"Verify KVM Installation","text":"<pre><code># Check if KVM modules are loaded\nlsmod | grep kvm\n\n# Expected output for AMD:\n# kvm_amd               155648  10\n# kvm                  1146880  1 kvm_amd\n# irqbypass              16384  26 kvm\n# ccp                   118784  1 kvm_amd\n\n# Expected output for Intel:\n# kvm_intel             294912  6\n# kvm                  1146880  1 kvm_intel\n\n# Verify libvirt service\nsystemctl status libvirtd\n\n# Test virtualization capabilities\nvirt-host-validate\n</code></pre> <p>Expected <code>virt-host-validate</code> output: <pre><code>  QEMU: Checking for hardware virtualization                 : PASS\n  QEMU: Checking if device /dev/kvm exists                   : PASS\n  QEMU: Checking if device /dev/kvm is accessible           : PASS\n  QEMU: Checking if device /dev/vhost-net exists            : PASS\n   LXC: Checking for Linux &gt;= 2.6.26                        : PASS\n</code></pre></p>"},{"location":"docs/getting-started/prerequisites/#verify-network-configuration","title":"Verify Network Configuration","text":"<pre><code># Check default libvirt network\nvirsh net-list --all\n\n# Expected output:\n# Name      State    Autostart   Persistent\n# default   active   yes         yes\n\n# If default network is not active:\nvirsh net-start default\nvirsh net-autostart default\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#verify-storage","title":"Verify Storage","text":"<pre><code># Check available disk space\ndf -h /var/lib/libvirt/images\n\n# Verify libvirt storage pools\nvirsh pool-list --all\n</code></pre>"},{"location":"docs/getting-started/prerequisites/#next-steps","title":"Next Steps","text":"<p>Once all prerequisites are met and verified:</p> <ol> <li>Infrastructure Deployment - Deploy VMs and networking</li> <li>Kubernetes Setup - Install and configure k0s cluster</li> </ol>"},{"location":"docs/storage/deployment/","title":"Ceph Deployment Guide","text":""},{"location":"docs/storage/deployment/#overview","title":"Overview","text":"<p>This guide provides step-by-step instructions for deploying a production-ready Ceph storage cluster using Ansible automation. The deployment follows a phased approach for better control, debugging, and reliability.</p>"},{"location":"docs/storage/deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/storage/deployment/#infrastructure-requirements","title":"Infrastructure Requirements","text":"Component Specification Notes Nodes 3x Virtual Machines ceph01, ceph02, ceph03 Memory 8GB per node Minimum for production workloads vCPUs 2 per node Adequate for test/dev environments System Disk 50GB per node OS and container images OSD Disk 100GB per node (<code>/dev/vdb</code>) Primary data storage DB/WAL Disk 50GB per node (<code>/dev/vdc</code>) Metadata and write-ahead logs"},{"location":"docs/storage/deployment/#network-configuration","title":"Network Configuration","text":"<pre><code>graph LR\n    subgraph \"Node Network Layout\"\n        subgraph \"ceph01\"\n            Eth0_1[eth0&lt;br/&gt;192.168.10.20&lt;br/&gt;Management]\n            Eth1_1[eth1&lt;br/&gt;192.168.11.20&lt;br/&gt;Cluster]\n        end\n\n        subgraph \"ceph02\"\n            Eth0_2[eth0&lt;br/&gt;192.168.10.21&lt;br/&gt;Management]\n            Eth1_2[eth1&lt;br/&gt;192.168.11.21&lt;br/&gt;Cluster]\n        end\n\n        subgraph \"ceph03\"\n            Eth0_3[eth0&lt;br/&gt;192.168.10.22&lt;br/&gt;Management]\n            Eth1_3[eth1&lt;br/&gt;192.168.11.22&lt;br/&gt;Cluster]\n        end\n    end\n\n    PublicBridge[virbr1&lt;br/&gt;192.168.10.0/24]\n    ClusterBridge[virbr2&lt;br/&gt;192.168.11.0/24]\n\n    Eth0_1 --- PublicBridge\n    Eth0_2 --- PublicBridge\n    Eth0_3 --- PublicBridge\n\n    Eth1_1 --- ClusterBridge\n    Eth1_2 --- ClusterBridge\n    Eth1_3 --- ClusterBridge</code></pre>"},{"location":"docs/storage/deployment/#software-prerequisites","title":"Software Prerequisites","text":""},{"location":"docs/storage/deployment/#control-node-requirements","title":"Control Node Requirements","text":"Software Version Purpose Ansible 2.12+ Infrastructure automation Python 3.8+ Ansible runtime dependency Collections <code>ceph.automation</code>, <code>community.general</code>, <code>ansible.posix</code> Ansible modules SSH Access Key-based Passwordless access to all nodes"},{"location":"docs/storage/deployment/#target-system-requirements","title":"Target System Requirements","text":"Component Requirement OS Debian 12 Architecture x86_64 Container Runtime Podman Time Sync chrony"},{"location":"docs/storage/deployment/#pre-deployment-setup","title":"Pre-Deployment Setup","text":""},{"location":"docs/storage/deployment/#1-prepare-control-node","title":"1. Prepare Control Node","text":""},{"location":"docs/storage/deployment/#install-ansible-and-dependencies","title":"Install Ansible and Dependencies","text":"<pre><code># Install Ansible and Python dependencies\npip3 install ansible\n\n# Install required Ansible collections\nansible-galaxy install -r requirements.yaml\n\n# Verify collection installation\nansible-galaxy collection list | grep -E \"(ceph|community|ansible)\"\n</code></pre>"},{"location":"docs/storage/deployment/#2-pre-deployment-validation","title":"2. Pre-deployment Validation","text":""},{"location":"docs/storage/deployment/#test-ansible-connectivity","title":"Test Ansible Connectivity","text":"<pre><code># Navigate to Ansible directory\ncd jarvis-kvm/ansible\n\n# Test connectivity to all nodes\nansible all -i inventories/development/hosts.yaml -m ping\n</code></pre> <p>Expected Output: <pre><code>ceph01 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre></p>"},{"location":"docs/storage/deployment/#validate-infrastructure","title":"Validate Infrastructure","text":"<pre><code># Verify disk layout on all nodes\nansible all -i inventories/development/hosts.yaml -m shell -a \"lsblk\"\n\n# Check network interface configuration\nansible all -i inventories/development/hosts.yaml -m shell -a \"ip addr show\"\n\n# Verify system resources\nansible all -i inventories/development/hosts.yaml -m shell -a \"free -h &amp;&amp; df -h\"\n</code></pre>"},{"location":"docs/storage/deployment/#deployment-process","title":"Deployment Process","text":"<p>The deployment follows a phased approach for reliability and debugging:</p>"},{"location":"docs/storage/deployment/#phase-1-system-preparation","title":"Phase 1: System Preparation","text":"<pre><code># Configure system prerequisites and repositories\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags preflight\n</code></pre>"},{"location":"docs/storage/deployment/#what-phase-1-accomplishes","title":"What Phase 1 Accomplishes","text":"Task Description DNS Resolution Configures <code>/etc/hosts</code> with cluster node mappings Time Sync Installs and configures chrony for cluster synchronization Container Runtime Installs Podman for containerized Ceph services Repositories Adds Ceph community repositories Base Packages Installs cephadm, firewalld, and dependencies Network Config Prepares firewall rules for Ceph services"},{"location":"docs/storage/deployment/#verification","title":"Verification","text":"<pre><code># Verify time synchronization\nansible all -i inventories/development/hosts.yaml -m shell -a \"chronyc sources -v\"\n\n# Check Podman installation\nansible all -i inventories/development/hosts.yaml -m shell -a \"podman --version\"\n</code></pre> <p>Note</p> <p>Recommended: Use local NTP server for production environment</p>"},{"location":"docs/storage/deployment/#phase-2-cluster-bootstrap","title":"Phase 2: Cluster Bootstrap","text":"<pre><code># Initialize the Ceph cluster on the admin node\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags bootstrap --ask-vault-pass\n</code></pre> <p>Info</p> <p>All variables are stored under <code>inventories/development/group_vars</code>. Use this command to encrypt and view the encrypted vault: <pre><code>ansible-vault encrypt inventories/development/group_vars/admin.yaml\nansible-vault view inventories/development/group_vars/admin.yaml\n\n# dashboard_user: admin\n# dashboard_password: b0b0yb0ys3cr3t\n# fsid: 47c6b1da-6271-43a2-9e52-50183ee3fa7e\n</code></pre></p>"},{"location":"docs/storage/deployment/#bootstrap-process","title":"Bootstrap Process","text":"Step Action Details 1. Cluster Init Initialize cluster on ceph01 FSID: <code>47c6b1da-6271-43a2-9e52-50183ee3fa7e</code> 2. Network Config Configure dual networks Public: <code>192.168.10.0/24</code>, Cluster: <code>192.168.11.0/24</code> 3. Dashboard Setup Enable management dashboard HTTPS on port 8443 with admin user 4. Base Config Apply cluster-wide settings Performance tuning and security"},{"location":"docs/storage/deployment/#verification_1","title":"Verification","text":"<pre><code># Check initial cluster status\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph -s\"\n</code></pre> <p>Expected Output: <pre><code>cluster:\n  id:     47c6b1da-6271-43a2-9e52-50183ee3fa7e\n  health: HEALTH_WARN (1 failed)\n\nservices:\n  mon: 1 daemons, quorum ceph01 (age 5m)\n  mgr: ceph01.abcdef(active, since 4m)\n</code></pre></p> <p>Note</p> <p><code>HEALTH_WARN</code> is expected at this stage as additional services are not yet deployed.</p>"},{"location":"docs/storage/deployment/#phase-3-cluster-expansion","title":"Phase 3: Cluster Expansion","text":"<pre><code># Distribute cephadm SSH keys between nodes and add all nodes to cluster with appropriate labels\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags add_nodes --ask-vault-pass\n</code></pre> <p>Info</p> <p>Applied node roles: - ceph01: <code>_admin</code>, <code>mon</code>, <code>mgr</code>, <code>osd</code>, <code>rgw</code> - ceph02: <code>mon</code>, <code>mgr</code>, <code>osd</code>, <code>rgw</code> - ceph03: <code>mon</code>, <code>mgr</code>, <code>osd</code>, <code>rgw</code></p>"},{"location":"docs/storage/deployment/#phase-4-service-deployment","title":"Phase 4: Service Deployment","text":""},{"location":"docs/storage/deployment/#step-41-core-services","title":"Step 4.1: Core Services","text":"<pre><code># Deploy MON, MGR, and RGW services\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags deploy_services --ask-vault-pass\n</code></pre> <p>Info</p> <p>Deployed services: - 3x MON: Cluster state management - 3x MGR: Cluster management and dashboard - 3x RGW: Object storage gateway</p>"},{"location":"docs/storage/deployment/#step-42-osd-deployment","title":"Step 4.2: OSD Deployment","text":"<pre><code># Configure and deploy Object Storage Daemons\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags configure_osds --ask-vault-pass\n</code></pre> <p>Info</p> <p>OSD configuration: - Data Device: <code>/dev/vdb</code> (100GB per node) - DB Device: <code>/dev/vdc</code> (50GB per node) - Total OSDs: 6 (2 per node)</p>"},{"location":"docs/storage/deployment/#phase-5-storage-configuration","title":"Phase 5: Storage Configuration","text":"<pre><code># Create storage pools for different use cases\nansible-playbook -i inventories/development/hosts.yaml site.yaml --tags configure_pools --ask-vault-pass\n</code></pre> <p>Info</p> <p>Created some ceph pools: - kubernetes - rbd_ec - data pool for RGW</p>"},{"location":"docs/storage/deployment/#complete-deployment","title":"Complete Deployment","text":""},{"location":"docs/storage/deployment/#single-command-deployment","title":"Single Command Deployment","text":"<p>For experienced users or automated deployments:</p> <pre><code># Deploy entire cluster in one run\nansible-playbook -i inventories/development/hosts.yaml site.yaml --ask-vault-pass\n\n# Deploy with verbose output for troubleshooting\nansible-playbook -i inventories/development/hosts.yaml site.yaml --ask-vault-pass -v\n</code></pre> <p>Note</p> <p>Recommended: Use phased deployment for first-time deployments or troubleshooting.</p>"},{"location":"docs/storage/deployment/#deployment-verification","title":"Deployment Verification","text":""},{"location":"docs/storage/deployment/#cluster-health-check","title":"Cluster Health Check","text":"<pre><code># Basic health verification\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph -s\"\n</code></pre> <p>Expected Output (Healthy Cluster): <pre><code>cluster:\n  id:     47c6b1da-6271-43a2-9e52-50183ee3fa7e\n  health: HEALTH_OK\n\nservices:\n  mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 2h)\n  mgr: ceph01.abcdef(active, since 2h), standbys: ceph02.ghijkl, ceph03.mnopqr\n  osd: 6 osds: 6 up (since 2h), 6 in (since 2h)\n  rgw: 3 daemons active (3 hosts, 1 zones)\n\ndata:\n  pools:   9 pools, 193 pgs\n  objects: 229 objects, 454 KiB\n  usage:   150 GiB used, 300 GiB / 450 GiB avail\n  pgs:     193 active+clean\n</code></pre></p>"},{"location":"docs/storage/deployment/#service-status-verification","title":"Service Status Verification","text":"<pre><code># Check all deployed services\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph orch ps\"\n\n# Verify service placement\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph orch ls\"\n</code></pre>"},{"location":"docs/storage/deployment/#storage-pool-verification","title":"Storage Pool Verification","text":"<pre><code># List all pools with details\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph osd pool ls detail\"\n\n# Check storage usage by pool\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph df\"\n\n# Verify OSD tree\nansible admin -i inventories/development/hosts.yaml -b -m shell -a \"ceph osd tree\"\n</code></pre>"},{"location":"docs/storage/deployment/#post-deployment-configuration","title":"Post-Deployment Configuration","text":""},{"location":"docs/storage/deployment/#dashboard-access","title":"Dashboard Access","text":"Setting Value URL https://192.168.10.20:8443 Username <code>admin</code> Password (configured in Ansible vault) Certificate Self-signed (accept browser warning)"},{"location":"docs/storage/deployment/#security-considerations","title":"Security Considerations","text":"<pre><code># Change default dashboard password\nceph dashboard ac-user-set-password admin &lt;new-password&gt;\n\n# Enable SSL certificate (optional)\nceph dashboard set-ssl-certificate -i /path/to/certificate.crt\nceph dashboard set-ssl-certificate-key -i /path/to/private.key\n</code></pre>"},{"location":"docs/storage/deployment/#next-steps","title":"Next Steps","text":"<p>After successful deployment:</p> <ol> <li>Configure Kubernetes Integration - Set up CSI driver for dynamic provisioning</li> <li>Create S3 Users - Configure object storage access</li> </ol>"},{"location":"docs/storage/overview/","title":"Ceph Storage Overview","text":""},{"location":"docs/storage/overview/#introduction","title":"Introduction","text":"<p>This documentation covers the Ceph distributed storage system deployed in the infra-lab environment. Ceph provides a unified storage platform that delivers block, object, and file storage services with enterprise-grade reliability, scalability, and performance.</p>"},{"location":"docs/storage/overview/#architecture-summary","title":"Architecture Summary","text":"<pre><code>graph TB\n    subgraph \"Ceph Storage Cluster\"\n        subgraph \"Node Configuration\"\n            Ceph01[ceph01&lt;br/&gt;192.168.10.20&lt;br/&gt;Admin + All Services]\n            Ceph02[ceph02&lt;br/&gt;192.168.10.21&lt;br/&gt;MON + MGR + OSD + RGW]\n            Ceph03[ceph03&lt;br/&gt;192.168.10.22&lt;br/&gt;MON + MGR + OSD + RGW]\n        end\n\n        subgraph \"Core Services\"\n            MON[3x MON&lt;br/&gt;Cluster State]\n            MGR[3x MGR&lt;br/&gt;Management]\n            OSD[6x OSD&lt;br/&gt;Storage Daemons]\n            RGW[3x RGW&lt;br/&gt;Object Gateway]\n        end\n\n        subgraph \"Storage Pools\"\n            K8sPool[kubernetes&lt;br/&gt;Replicated Pool&lt;br/&gt;K8s PVs]\n            RBDPool[rbd_ec&lt;br/&gt;Erasure Coded&lt;br/&gt;General Use]\n            RGWData[rgw.buckets.data&lt;br/&gt;Erasure Coded&lt;br/&gt;Object Data]\n            RGWIndex[rgw.buckets.index&lt;br/&gt;Replicated&lt;br/&gt;Object Metadata]\n        end\n    end\n\n    subgraph \"Client Access\"\n        K8sCluster[Kubernetes Cluster&lt;br/&gt;RBD Volumes]\n        S3Client[S3/Swift Clients&lt;br/&gt;Object Storage]\n        Dashboard[Ceph Dashboard&lt;br/&gt;Management UI]\n    end\n\n    subgraph \"Networks\"\n        PublicNet[Public Network&lt;br/&gt;192.168.10.0/24&lt;br/&gt;Client Access]\n        ClusterNet[Cluster Network&lt;br/&gt;192.168.11.0/24&lt;br/&gt;Internal Traffic]\n    end\n\n    %% Connections\n    Ceph01 --- MON\n    Ceph02 --- MON\n    Ceph03 --- MON\n\n    MON --- MGR\n    MGR --- OSD\n    OSD --- K8sPool\n    OSD --- RBDPool\n    RGW --- RGWData\n    RGW --- RGWIndex\n\n    K8sCluster --&gt; K8sPool\n    S3Client --&gt; RGW\n    Dashboard --&gt; MGR\n\n    %% Network connections\n    PublicNet --- Ceph01\n    PublicNet --- Ceph02\n    PublicNet --- Ceph03\n\n    ClusterNet -.-&gt; Ceph01\n    ClusterNet -.-&gt; Ceph02\n    ClusterNet -.-&gt; Ceph03\n\n    %% Styling\n    classDef node fill:#e8f5e8,stroke:#2e7d32,color:#000\n    classDef service fill:#fff3e0,stroke:#ef6c00,color:#000\n    classDef pool fill:#e3f2fd,stroke:#1565c0,color:#000\n    classDef client fill:#fce4ec,stroke:#c2185b,color:#000\n    classDef network fill:#f3e5f5,stroke:#6a1b9a,color:#000\n\n    class Ceph01,Ceph02,Ceph03 node\n    class MON,MGR,OSD,RGW service\n    class K8sPool,RBDPool,RGWData,RGWIndex pool\n    class K8sCluster,S3Client,Dashboard client\n    class PublicNet,ClusterNet network</code></pre>"},{"location":"docs/storage/overview/#cluster-specifications","title":"Cluster Specifications","text":"Component Details Version Ceph Squid (Community Edition) Cluster ID <code>47c6b1da-6271-43a2-9e52-50183ee3fa7e</code> Nodes 3 (ceph01, ceph02, ceph03) Raw Storage 300GB (100GB per node) Usable Storage ~200GB (with 3x replication) Replication Factor 3x for critical data Erasure Coding k=2, m=1 (66% storage efficiency)"},{"location":"docs/storage/overview/#network-architecture","title":"Network Architecture","text":""},{"location":"docs/storage/overview/#dual-network-design","title":"Dual Network Design","text":"<p>Ceph uses separate networks for optimal performance and security:</p> Network Type CIDR Purpose Interface Public Network <code>192.168.10.0/24</code> Client access, management, monitors <code>eth0</code> Cluster Network <code>192.168.11.0/24</code> OSD replication, recovery, heartbeat <code>eth1</code> <p>Note</p> <p>The cluster network handles high-volume internal traffic, keeping client operations on the public network responsive.</p>"},{"location":"docs/storage/overview/#node-configuration","title":"Node Configuration","text":"Node Public IP Cluster IP Services ceph01 <code>192.168.10.20</code> <code>192.168.11.20</code> Admin, MON, MGR, OSD, RGW ceph02 <code>192.168.10.21</code> <code>192.168.11.21</code> MON, MGR, OSD, RGW ceph03 <code>192.168.10.22</code> <code>192.168.11.22</code> MON, MGR, OSD, RGW"},{"location":"docs/storage/overview/#storage-pools","title":"Storage Pools","text":"<p>The cluster provides different storage pools optimized for specific use cases:</p>"},{"location":"docs/storage/overview/#block-storage-pools","title":"Block Storage Pools","text":"Pool Name Type Replication/EC PGs Use Case <code>kubernetes</code> Replicated 3x 64 Kubernetes persistent volumes <code>rbd_ec</code> Erasure Coded k=2, m=1 64 General block storage (space-efficient)"},{"location":"docs/storage/overview/#object-storage-pools","title":"Object Storage Pools","text":"Pool Name Type Replication/EC PGs Use Case <code>default.rgw.buckets.data</code> Erasure Coded k=2, m=1 64 S3 object data storage <code>default.rgw.buckets.index</code> Replicated 3x 32 S3 bucket metadata and indexes <p>Pool Selection Guide: - Replicated pools: Better performance, higher storage overhead (3x) - Erasure coded pools: Lower storage overhead (1.5x), slightly higher latency</p>"},{"location":"docs/storage/overview/#access-methods","title":"Access Methods","text":""},{"location":"docs/storage/overview/#management-dashboard","title":"Management Dashboard","text":"<ul> <li>URL: https://192.168.10.20:8443</li> <li>Username: <code>admin</code></li> <li>Features: Cluster monitoring, pool management, OSD operations, performance metrics</li> </ul>"},{"location":"docs/storage/overview/#object-storage-s3","title":"Object Storage (S3)","text":"<ul> <li>Endpoint: http://192.168.10.20:80</li> <li>Compatibility: AWS S3 API compatible</li> <li>Features: Bucket management, access policies, multi-user support</li> </ul>"},{"location":"docs/storage/overview/#getting-started","title":"Getting Started","text":"<ol> <li>Deployment Guide - Complete installation and configuration</li> <li>Kubernetes Integration - CSI driver setup and usage examples</li> </ol>"},{"location":"docs/storage/integrations/kubernetes/","title":"Kubernetes Integration Guide","text":""},{"location":"docs/storage/integrations/kubernetes/#overview","title":"Overview","text":"<p>This guide provides comprehensive instructions for integrating Ceph storage with Kubernetes using the Ceph CSI (Container Storage Interface) driver. The integration enables dynamic provisioning of persistent volumes with support for RBD (block storage) and S3 compatible storage.</p>"},{"location":"docs/storage/integrations/kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ceph pool with name <code>kubernetes</code> (already created via ansible)</li> </ul>"},{"location":"docs/storage/integrations/kubernetes/#ceph-cluster-preparation","title":"Ceph Cluster Preparation","text":""},{"location":"docs/storage/integrations/kubernetes/#1-setup-client-authentication","title":"1. Setup Client Authentication","text":"<p>Create a dedicated Ceph user for Kubernetes with minimal required permissions:</p> <pre><code># Create the client with specific permissions\nceph auth get-or-create client.kubernetes \\\n  mon 'profile rbd' \\\n  osd 'profile rbd pool=kubernetes' \\\n  mgr 'profile rbd pool=kubernetes'\n</code></pre> <p>Example output: <pre><code>[client.kubernetes]\n    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==\n</code></pre></p>"},{"location":"docs/storage/integrations/kubernetes/#2-verify-cluster-information","title":"2. Verify Cluster Information","text":"<p>Collect the following information needed for CSI configuration:</p> <pre><code># Get monitor addresses\nceph mon dump\n\n# Get cluster FSID\nceph status\n\n# Verify client access\nceph auth get client.kubernetes\n</code></pre>"},{"location":"docs/storage/integrations/kubernetes/#install-ceph-csi-driver","title":"Install Ceph CSI Driver","text":""},{"location":"docs/storage/integrations/kubernetes/#1-create-namespace","title":"1. Create Namespace","text":"<p>Create a dedicated namespace for the CSI driver:</p> <pre><code>kubectl create namespace ceph-csi-rbd\n</code></pre>"},{"location":"docs/storage/integrations/kubernetes/#2-install-csi-driver-using-helm-and-kustomize","title":"2. Install CSI Driver using Helm and Kustomize","text":"<p>Create a values file for the Helm chart:</p> <pre><code># values.yaml\ncsiConfig:\n  - clusterID: \"YOUR_CLUSTER_FSID\"\n    monitors:\n      - \"MONITOR_IP_1:6789\"\n      - \"MONITOR_IP_2:6789\" \n      - \"MONITOR_IP_3:6789\"\n\nsecret:\n  create: true\n  name: csi-rbd-secret\n  userID: kubernetes\n  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==\n\nstorageClass:\n  create: true\n  name: ceph-rbd\n  clusterID: YOUR_CLUSTER_FSID\n  pool: kubernetes\n  reclaimPolicy: Delete\n  allowVolumeExpansion: true\n  mountOptions: []\n\nprovisioner:\n  name: rbd.csi.ceph.com\n  replicaCount: 2\n\nnodeplugin:\n  name: csi-rbdplugin\n</code></pre> <p>Alternative installation method using the referenced helm chart configuration here</p>"},{"location":"docs/storage/integrations/kubernetes/#3-verify-installation","title":"3. Verify Installation","text":"<p>Check that all CSI components are running:</p> <pre><code># Check CSI pods\nkubectl get pods -n ceph-csi-rbd\n\n# Check CSI driver registration\nkubectl get csidrivers\n\n# Check storage class\nkubectl get storageclass\n</code></pre>"},{"location":"docs/storage/integrations/kubernetes/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/storage/integrations/kubernetes/#basic-persistentvolumeclaim","title":"Basic PersistentVolumeClaim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rbd-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: ceph-rbd\n</code></pre>"},{"location":"docs/storage/integrations/kubernetes/#pod-using-ceph-rbd-volume","title":"Pod Using Ceph RBD Volume","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ceph-rbd-pod\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypd\n      mountPath: /var/lib/www/html\n  volumes:\n  - name: mypd\n    persistentVolumeClaim:\n      claimName: rbd-pvc\n      readOnly: false\n</code></pre>"},{"location":"docs/storage/integrations/kubernetes/#references","title":"References","text":"<ul> <li>Ceph CSI Official Documentation</li> <li>Ceph RBD Documentation</li> </ul>"}]}